{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBOCb6iRchvQ",
        "outputId": "f043dc67-8e53-4aaf-8891-86a83720df93",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.0-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.4.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "#necessary installs\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install  scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmQ0l7gHLPZ"
      },
      "outputs": [],
      "source": [
        "#from\n",
        "from google.colab import drive\n",
        "from huggingface_hub import notebook_login\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, BertModel, DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import concurrent.futures\n",
        "import torch\n",
        "import time\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "c5d3214e563b4212adce214338ded1ad",
            "88a6a9d946f84f1fbea461ba961e6cea",
            "9ccac6d0e4754863bc98501e521f9493",
            "ccaa5c38502c437897d9e918926a8f35",
            "55da9c38e5704fae840a0292e6d68e3d",
            "cf5f355b9bc54e4ab31f719d55bcbe78",
            "bce3a63611334d659805a3ef9ea961d2",
            "38b951013c7e408cb84e5d4f57a90369",
            "22543734c08149e7865a32553d314765",
            "c5c6ca5d896b4fa7aa1bfc7e867c6a6e",
            "9b3cd07b72294948a1c7a5854f15a847",
            "574c79a2b2e14d808fcdc7cbff50d1ca",
            "1b0ede8519734344ad1713aea37c3ee7",
            "64a50aa7acec467ebeaa8837bd710d3d",
            "aa08eb0bc4f8474c986157d6619136e0",
            "ba821c1e12ec414b894612a89fa5df89",
            "08fb12287bbf4f82b7fd32c6164844fc",
            "5f127e8a3d034b23a7e9eec90485e7ca",
            "c825618c22b0409dab0e3a2fde6eed9f",
            "7caa550cef5e4c69958bc025d2d01e8c"
          ]
        },
        "id": "27vg27YeHPzA",
        "outputId": "4965895b-02f5-4afb-b880-15aba3571ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5d3214e563b4212adce214338ded1ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape:  (159571, 8)\n",
            "Test shape:  (153164, 2)\n"
          ]
        }
      ],
      "source": [
        "#paths\n",
        "drive.mount('/content/drive')\n",
        "notebook_login()\n",
        "\n",
        "dataPATH = '/content/drive/MyDrive/LINKUP_DATA'\n",
        "\n",
        "trainPATH = f\"{dataPATH}/train.csv\"\n",
        "\n",
        "testPATH=f\"{dataPATH}/test.csv\"\n",
        "\n",
        "#Load data sets\n",
        "\n",
        "df_train = pd.read_csv(trainPATH)\n",
        "\n",
        "df_test = pd.read_csv(testPATH)\n",
        "\n",
        "#checking shape\n",
        "\n",
        "print(\"Training shape: \",df_train.shape) #expected 8: id, text, and 6 categories\n",
        "\n",
        "print(\"Test shape: \",df_test.shape) #expected 2: id & text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbsfQizQW4E7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca698999-b9e5-42fa-fb3d-e8b74845ea4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "New Label Distribution:\n",
            " toxic            136995\n",
            "severe_toxic      45658\n",
            "obscene          110301\n",
            "threat            29616\n",
            "insult           108440\n",
            "identity_hate     41943\n",
            "dtype: int64\n",
            "Training Samples: 229353, Validation Samples: 57339\n",
            "Tokenization Complete\n",
            "DataLoader Initialized\n",
            "\n",
            "Model and Optimizer Initialized!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Load Tokenizer & Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\", flush=True)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "# bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
        "bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "\n",
        "#Balance Dataset Using Oversampling\n",
        "def balance_dataset(df):\n",
        "    label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "    df_majority = df[df[label_columns].sum(axis=1) == 0]\n",
        "\n",
        "    balanced_dfs = [df_majority]\n",
        "    for label in label_columns:\n",
        "        df_minority = df[df[label] == 1]\n",
        "        df_minority_upsampled = resample(df_minority, replace=True,\n",
        "                                         n_samples=len(df_majority) // len(label_columns),\n",
        "                                         random_state=42)\n",
        "        balanced_dfs.append(df_minority_upsampled)\n",
        "\n",
        "    df_balanced = pd.concat(balanced_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    return df_balanced\n",
        "\n",
        "df_train_balanced = balance_dataset(df_train)\n",
        "print(\"New Label Distribution:\\n\", df_train_balanced.iloc[:, 2:].sum(), flush=True)\n",
        "\n",
        "#Convert Labels to Multi-Hot Encoded Vectors\n",
        "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y_train = df_train_balanced[label_columns].values\n",
        "\n",
        "#Split Data into Train & Validation Sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(df_train_balanced[\"comment_text\"], y_train,\n",
        "                                                  test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training Samples: {len(X_train)}, Validation Samples: {len(X_val)}\", flush=True)\n",
        "\n",
        "# Convert Text to Tokenized Tensors\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    encodings = tokenizer(\n",
        "        list(texts),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encodings[\"input_ids\"], encodings[\"attention_mask\"]\n",
        "\n",
        "# def tokenize_texts(texts, tokenizer, max_length=128, batch_size=50000):\n",
        "#     input_ids_list = []\n",
        "#     attention_masks_list = []\n",
        "\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     for i in range(0, len(texts), batch_size):\n",
        "#         batch_texts = texts[i:i+batch_size]\n",
        "#         encodings = tokenizer(\n",
        "#             list(batch_texts),\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             max_length=max_length,\n",
        "#             return_tensors=\"pt\"\n",
        "#         )\n",
        "#         input_ids_list.append(encodings[\"input_ids\"])\n",
        "#         attention_masks_list.append(encodings[\"attention_mask\"])\n",
        "\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"Processed {i + len(batch_texts)} / {len(texts)} samples - Time elapsed: {elapsed_time:.2f} sec\", flush=True)\n",
        "\n",
        "#     print(f\"\\nTokenization complete! Total time: {time.time() - start_time:.2f} sec\", flush=True)\n",
        "\n",
        "#     return torch.cat(input_ids_list, dim=0), torch.cat(attention_masks_list, dim=0)\n",
        "\n",
        "\n",
        "#Tokenize Training and Validation Sets\n",
        "X_train_tokens, X_train_masks = tokenize_texts(X_train, tokenizer)\n",
        "X_val_tokens, X_val_masks = tokenize_texts(X_val, tokenizer)\n",
        "print(\"Tokenization Complete\", flush=True)\n",
        "\n",
        "#Convert labels to tensors\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for Batches\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tokens, X_train_masks, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tokens, X_val_masks, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)  # Disable multiprocessing for debugging\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "print(\"DataLoader Initialized\", flush=True)\n",
        "\n",
        "# # Multi-Label Classification Model\n",
        "# class MultiLabelClassifier(nn.Module):\n",
        "#     def __init__(self, output_dim):\n",
        "#         super(MultiLabelClassifier, self).__init__()\n",
        "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")  # Load BERT model\n",
        "#         self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "#         self.sigmoid = nn.Sigmoid()  # Multi-label classification\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         pooled_output = outputs.pooler_output  # Get pooled BERT representation\n",
        "#         return self.sigmoid(self.fc(pooled_output))  # Convert logits to probabilities\n",
        "\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()  # Multi-label classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.sigmoid(self.fc(pooled_output))  # Convert logits to probabilities\n",
        "\n",
        "\n",
        "# Model Setup\n",
        "output_dim = y_train_tensor.shape[1]  # Number of labels\n",
        "model = MultiLabelClassifier(output_dim).to(device)\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy for multi-label classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "#Debugging Assertion to Confirm Execution Reaches This Point\n",
        "print(\"\\nModel and Optimizer Initialized!\", flush=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaomYjH9t8Dm",
        "outputId": "e2f35884-5322-421f-a185-8c37abfadc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3 starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 1792/1792 [21:33<00:00,  1.39it/s, Loss=0.0594, AUC=0.9377, F1=0.9119, Precision=0.9185, Recall=0.9060]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 | Train Loss: 0.1065 | AUC: 0.9896 | F1-score: 0.9119 | Precision: 0.9185 | Recall: 0.9060\n",
            "\n",
            "Epoch 2/3 starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 1792/1792 [21:32<00:00,  1.39it/s, Loss=0.0245, AUC=0.9875, F1=0.9829, Precision=0.9807, Recall=0.9852]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3 | Train Loss: 0.0346 | AUC: 0.9987 | F1-score: 0.9829 | Precision: 0.9807 | Recall: 0.9852\n",
            "\n",
            "Epoch 3/3 starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 1792/1792 [21:27<00:00,  1.39it/s, Loss=0.00776, AUC=0.9935, F1=0.9915, Precision=0.9907, Recall=0.9923]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3 | Train Loss: 0.0185 | AUC: 0.9995 | F1-score: 0.9915 | Precision: 0.9907 | Recall: 0.9923\n",
            "\n",
            "Model Training Complete!\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Define Class Labels for Better Display\n",
        "label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} starting...\", flush=True)\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        #Store outputs & labels for metric calculation\n",
        "        all_labels.append(labels.cpu().detach().numpy())\n",
        "        all_outputs.append(outputs.cpu().detach().numpy())\n",
        "\n",
        "        #Calculate metrics after each batch\n",
        "        labels_np = np.vstack(all_labels)\n",
        "        outputs_np = np.vstack(all_outputs).round()\n",
        "\n",
        "        auc_score = roc_auc_score(labels_np, outputs_np, average=\"macro\")\n",
        "        f1 = f1_score(labels_np, outputs_np, average=\"macro\")\n",
        "        precision = precision_score(labels_np, outputs_np, average=\"macro\", zero_division=0)\n",
        "        recall = recall_score(labels_np, outputs_np, average=\"macro\", zero_division=0)\n",
        "\n",
        "        #Update the progress bar with metrics\n",
        "        progress_bar.set_postfix({\n",
        "            \"Loss\": loss.item(),\n",
        "            \"AUC\": f\"{auc_score:.4f}\",\n",
        "            \"F1\": f\"{f1:.4f}\",\n",
        "            \"Precision\": f\"{precision:.4f}\",\n",
        "            \"Recall\": f\"{recall:.4f}\"\n",
        "        })\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    #Compute Final Metrics for the Epoch\n",
        "    all_labels = np.vstack(all_labels)\n",
        "    all_outputs = np.vstack(all_outputs)\n",
        "\n",
        "    auc_score = roc_auc_score(all_labels, all_outputs, average=\"macro\")\n",
        "    f1 = f1_score(all_labels, all_outputs.round(), average=\"macro\")\n",
        "    precision = precision_score(all_labels, all_outputs.round(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_outputs.round(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | \"\n",
        "          f\"AUC: {auc_score:.4f} | F1-score: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\",\n",
        "          flush=True)\n",
        "\n",
        "print(\"\\nModel Training Complete!\", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3YFJGEJ5gHD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "model_save_dir = \"/content/drive/MyDrive/LINKUP_modelDIS\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "model_save_path = os.path.join(model_save_dir, \"LINKUP_modelDIS.pth\")\n",
        "torch.save(model, model_save_path)\n",
        "print(f\" Model saved at {model_save_path}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94x1QSUt8ADl",
        "outputId": "ce1216b5-9f24-4577-8165-1dac84ae0bec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-62c5b61d5126>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_save_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded\n",
            "Tokenization Complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1197/1197 [03:22<00:00,  5.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Predictions saved to: /content/drive/MyDrive/LINKUP_model2/test_predictions2.csv\n"
          ]
        }
      ],
      "source": [
        "#Load Model\n",
        "model = torch.load(model_save_path)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded\")\n",
        "\n",
        "#Tokenize Test Data\n",
        "X_test_tokens, X_test_masks = tokenize_texts(df_test[\"comment_text\"], tokenizer)\n",
        "print(\"Tokenization Complete\")\n",
        "\n",
        "# Test DataLoader\n",
        "batch_size = 128\n",
        "test_dataset = TensorDataset(X_test_tokens, X_test_masks)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# Run Model on Test Data\n",
        "\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids, attention_mask = [x.to(device) for x in batch]\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        all_predictions.append(outputs.cpu().numpy())\n",
        "\n",
        "# Convert Predictions to Labels\n",
        "all_predictions = np.vstack(all_predictions)\n",
        "threshold = 0.5\n",
        "binary_predictions = (all_predictions >= threshold).astype(int)\n",
        "\n",
        "#Save Predictions\n",
        "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "df_test_results = df_test.copy()\n",
        "df_test_results[label_columns] = binary_predictions\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/LINKUP_model2/test_predictions2.csv\"\n",
        "df_test_results.to_csv(output_path, index=False)\n",
        "print(f\"\\n Predictions saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5G-41In4n04",
        "outputId": "12e9524b-fdfe-4432-e7c7-c5e70d9b7093",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-bd40fb1f17fb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_save_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Tokenizer loaded!\n",
            "Tokenization complete! Shape: torch.Size([7, 128])\n",
            "Processed Predictions: [[0.70951307 0.50007236 0.5000425  0.50091904 0.62429357 0.5000412 ]\n",
            " [0.5005124  0.5000309  0.5000346  0.50003064 0.5001662  0.50002295]\n",
            " [0.730574   0.50017583 0.50074923 0.73093516 0.5006667  0.5006944 ]\n",
            " [0.730562   0.50003284 0.6329238  0.50002927 0.50415385 0.5000302 ]\n",
            " [0.50018156 0.5000454  0.5000169  0.50004464 0.5000871  0.5000303 ]\n",
            " [0.50049883 0.5000363  0.5000093  0.50471526 0.5002195  0.50002277]\n",
            " [0.50015575 0.50004303 0.5000139  0.500046   0.50004935 0.5000331 ]]\n",
            "\n",
            "Text: You are the worst person ever!\n",
            "Predicted Categories: ['toxic', 'insult']\n",
            "\n",
            "Text: I hope you have an amazing day!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I'll kill u!\n",
            "Predicted Categories: ['toxic', 'threat']\n",
            "\n",
            "Text: This is absolute garbage!\n",
            "Predicted Categories: ['toxic', 'obscene']\n",
            "\n",
            "Text: What a wonderful place to visit!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I love you\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Hello, my name is Anthony\n",
            "Predicted Categories: ['None']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define Device (Use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Model\n",
        "model_save_path = \"/content/drive/MyDrive/LINKUP_model2/LINKUP_model2.pth\"\n",
        "model = torch.load(model_save_path, map_location=device)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(\"Tokenizer loaded!\")\n",
        "\n",
        "# Define Sample Test Sentences\n",
        "sample_texts = [\n",
        "    \"You are the worst person ever!\",  # Likely toxic, insult\n",
        "    \"I hope you have an amazing day!\",  # Not toxic\n",
        "    \"I'll kill u!\",  # Likely threat\n",
        "    \"This is absolute garbage!\",  # Possibly toxic, insult\n",
        "    \"What a wonderful place to visit!\",  # Not toxic\n",
        "    \"I love you\",  # Should not be toxic\n",
        "    \"Hello, my name is Anthony\",  # Should not be toxic\n",
        "\n",
        "]\n",
        "\n",
        "# Function to Tokenize Input Texts\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
        "\n",
        "# Tokenize Sample Inputs\n",
        "X_sample_tokens, X_sample_masks = tokenize_texts(sample_texts, tokenizer)\n",
        "X_sample_tokens, X_sample_masks = X_sample_tokens.to(device), X_sample_masks.to(device)\n",
        "print(\"Tokenization complete! Shape:\", X_sample_tokens.shape)\n",
        "\n",
        "# Run Model on Sample Texts\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_sample_tokens, attention_mask=X_sample_masks)  # Get raw logits\n",
        "    predictions = torch.sigmoid(outputs).cpu().numpy()  # Apply sigmoid activation\n",
        "\n",
        "print(\"Processed Predictions:\", predictions)  # Debugging Step\n",
        "\n",
        "# Adjust Threshold to Reduce False Positives\n",
        "threshold = 0.6  # Test a higher threshold for better classification\n",
        "binary_predictions = (predictions >= threshold).astype(int)\n",
        "\n",
        "# Define Label Categories\n",
        "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "# Print Results\n",
        "for i, text in enumerate(sample_texts):\n",
        "    predicted_labels = [label_columns[j] for j in range(len(label_columns)) if binary_predictions[i][j] == 1]\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(\"Predicted Categories:\", predicted_labels if predicted_labels else [\"None\"])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0lHwuVi5a99",
        "outputId": "eb87ebe4-4a8c-4056-aa41-68aff67d83b2",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-dbffb481cfdc>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_save_path)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MultiLabelClassifier(\n",
              "  (bert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=6, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_save_path = \"/content/drive/MyDrive/LINKUP_modelDIS/LINKUP_modelDIS.pth\"\n",
        "model = torch.load(model_save_path)\n",
        "model.eval()  # Set to evaluation mode before further training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awyXeHv28Kbe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAU-KBRS1mix",
        "outputId": "0aa03e5a-2afe-4f27-9b07-e2ff2dcfc080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fine-Tuning Epoch 1/3 starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 1792/1792 [07:59<00:00,  3.73it/s, Loss=0.00734]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 | Train Loss: 0.0097 | AUC: 0.9999 | F1-score: 0.9956 | Precision: 0.9951 | Recall: 0.9960\n",
            "\n",
            "Fine-Tuning Epoch 2/3 starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 1792/1792 [07:59<00:00,  3.74it/s, Loss=0.00242]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3 | Train Loss: 0.0069 | AUC: 0.9999 | F1-score: 0.9967 | Precision: 0.9964 | Recall: 0.9971\n",
            "\n",
            "Fine-Tuning Epoch 3/3 starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 1792/1792 [07:59<00:00,  3.74it/s, Loss=0.00139]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3 | Train Loss: 0.0053 | AUC: 1.0000 | F1-score: 0.9973 | Precision: 0.9971 | Recall: 0.9975\n",
            "\n",
            "Fine-Tuning Complete!\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)  # Reduce learning rate for fine-tuning\n",
        "\n",
        "epochs = 3  # Adjust as needed\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nFine-Tuning Epoch {epoch+1}/{epochs} starting...\", flush=True)\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Store outputs & labels for metric calculation\n",
        "        all_labels.append(labels.cpu().detach().numpy())\n",
        "        all_outputs.append(outputs.cpu().detach().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Compute final metrics\n",
        "    all_labels = np.vstack(all_labels)\n",
        "    all_outputs = np.vstack(all_outputs)\n",
        "\n",
        "    auc_score = roc_auc_score(all_labels, all_outputs, average=\"macro\")\n",
        "    f1 = f1_score(all_labels, all_outputs.round(), average=\"macro\")\n",
        "    precision = precision_score(all_labels, all_outputs.round(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_outputs.round(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | \"\n",
        "          f\"AUC: {auc_score:.4f} | F1-score: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\",\n",
        "          flush=True)\n",
        "\n",
        "print(\"\\nFine-Tuning Complete!\", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK3F-xUyTSnS",
        "outputId": "0ee8d144-62c5-458d-aefe-6ba6a2266f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-Tuned Model saved at /content/drive/MyDrive/LINKUP_modelDIS_finetuned/LINKUP_modelDIS_finetuned.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model_save_dir = \"/content/drive/MyDrive/LINKUP_modelDIS_finetuned\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "model_save_path = os.path.join(model_save_dir, \"LINKUP_modelDIS_finetuned.pth\")\n",
        "torch.save(model, model_save_path)\n",
        "print(f\"Fine-Tuned Model saved at {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i80SK4G8ZFv8",
        "outputId": "f1c78eb3-1fda-4715-a1a0-28913f983716"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-bead620436aa>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_save_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned model loaded successfully!\n",
            "Tokenizer loaded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-bead620436aa>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_save_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-Tuned Model loaded successfully!\n",
            "Tokenization complete! Shape: torch.Size([20, 128])\n",
            "Processed Predictions: [[9.99902010e-01 5.83732268e-04 8.25856332e-05 1.58088806e-03\n",
            "  3.99102122e-01 1.40237054e-04]\n",
            " [6.35343094e-05 1.24760976e-04 4.28407620e-05 3.61446546e-05\n",
            "  4.28267173e-04 8.97139762e-05]\n",
            " [9.98806596e-01 8.26396572e-04 5.28100471e-04 9.99918938e-01\n",
            "  4.33454203e-04 1.40810921e-03]\n",
            " [9.99767125e-01 2.38211913e-04 2.97127277e-01 1.40648990e-04\n",
            "  6.19553612e-05 2.29523343e-04]\n",
            " [8.47046249e-06 2.46437878e-04 5.54715734e-05 1.68424725e-04\n",
            "  3.28943646e-03 2.71633558e-04]\n",
            " [3.75841931e-03 3.37589619e-04 2.30292295e-04 2.22650182e-04\n",
            "  4.17579431e-05 5.68618052e-05]\n",
            " [4.06776999e-05 9.96789604e-05 2.05902506e-05 1.90286097e-04\n",
            "  1.50358945e-04 6.28743874e-05]\n",
            " [9.99926567e-01 3.22772656e-04 8.26042354e-01 1.24157144e-04\n",
            "  9.94333386e-01 8.96525762e-06]\n",
            " [1.76365502e-05 1.99300353e-04 4.31297558e-05 7.67349338e-05\n",
            "  6.36283366e-04 2.22271134e-04]\n",
            " [9.99707401e-01 2.08261190e-03 2.62759393e-04 9.91582453e-01\n",
            "  1.90169842e-04 5.81731438e-04]\n",
            " [1.73744818e-04 8.20300120e-05 1.51959102e-04 3.57155004e-05\n",
            "  5.97892737e-04 3.03738416e-05]\n",
            " [9.99956846e-01 7.52486376e-05 9.98600781e-01 7.53174099e-05\n",
            "  9.99772966e-01 2.58311135e-04]\n",
            " [2.92252353e-05 9.33094125e-05 1.03412865e-04 9.69016255e-05\n",
            "  4.65377743e-05 5.70778611e-05]\n",
            " [9.99644399e-01 7.11570263e-01 9.96730924e-01 9.99990225e-01\n",
            "  9.99976277e-01 5.94234385e-04]\n",
            " [9.99517918e-01 2.47835740e-03 9.61219339e-05 3.39941238e-03\n",
            "  9.99310613e-01 1.62640288e-02]\n",
            " [1.13692485e-01 4.09581589e-05 1.39430646e-04 2.18314581e-05\n",
            "  2.95527516e-05 4.12893996e-06]\n",
            " [9.83827591e-01 4.64504265e-05 3.73045012e-04 5.47004820e-05\n",
            "  9.90813076e-02 5.77411129e-06]\n",
            " [9.69646394e-01 2.14097799e-05 2.13642442e-03 2.35477000e-05\n",
            "  9.99586999e-01 9.80961263e-01]\n",
            " [9.99976873e-01 1.98935196e-02 9.99924660e-01 1.30171014e-04\n",
            "  9.99923587e-01 9.99850750e-01]\n",
            " [1.43462652e-02 1.39020785e-05 3.72405775e-04 5.07251616e-06\n",
            "  8.61870139e-05 1.33572585e-05]]\n",
            "\n",
            "Text: You are the worst person ever!\n",
            "Predicted Categories: ['toxic']\n",
            "\n",
            "Text: I hope you have an amazing day!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I'll kill u!\n",
            "Predicted Categories: ['toxic', 'threat']\n",
            "\n",
            "Text: This is absolute garbage!\n",
            "Predicted Categories: ['toxic']\n",
            "\n",
            "Text: What a wonderful place to visit!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I love you\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Hello, my name is Anthony\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Go to hell!\n",
            "Predicted Categories: ['toxic', 'obscene', 'insult']\n",
            "\n",
            "Text: I hope your day is amazing!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I'm going to destroy you.\n",
            "Predicted Categories: ['toxic', 'threat']\n",
            "\n",
            "Text: You are such a nice person.\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Shut up, you idiot.\n",
            "Predicted Categories: ['toxic', 'obscene', 'insult']\n",
            "\n",
            "Text: I disagree, but I respect your opinion.\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I hope you die you stupid ugly bitch\n",
            "Predicted Categories: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult']\n",
            "\n",
            "Text: you look fat\n",
            "Predicted Categories: ['toxic', 'insult']\n",
            "\n",
            "Text: what size is your dress? XXXXXL?\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: you're not very smart are you?\n",
            "Predicted Categories: ['toxic']\n",
            "\n",
            "Text: how many dogs have you eaten, asian coworker\n",
            "Predicted Categories: ['toxic', 'insult', 'identity_hate']\n",
            "\n",
            "Text: are you fucking homosexual\n",
            "Predicted Categories: ['toxic', 'obscene', 'insult', 'identity_hate']\n",
            "\n",
            "Text: coworker, should I kill myself?\n",
            "Predicted Categories: ['None']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path to saved model\n",
        "model_save_path = \"/content/drive/MyDrive/LINKUP_modelDIS_finetuned/LINKUP_modelDIS_finetuned.pth\"\n",
        "\n",
        "# Load the full model\n",
        "model = torch.load(model_save_path, map_location=device)\n",
        "model.to(device)\n",
        "model.eval()  # Set to evaluation mode\n",
        "print(\"Fine-tuned model loaded successfully!\")\n",
        "\n",
        "# Load Tokenizer for DistilBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(\"Tokenizer loaded!\")\n",
        "\n",
        "# Define MultiLabelClassifier (Same as Training Script)\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()  # Multi-label classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
        "        return self.sigmoid(self.fc(pooled_output))  # Convert logits to probabilities\n",
        "\n",
        "# Load Model\n",
        "model_save_path = \"/content/drive/MyDrive/LINKUP_modelDIS_finetuned/LINKUP_modelDIS_finetuned.pth\"\n",
        "output_dim = 6  # Ensure this matches your number of labels\n",
        "\n",
        "model = MultiLabelClassifier(output_dim)\n",
        "model = torch.load(model_save_path, map_location=device)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Fine-Tuned Model loaded successfully!\")\n",
        "\n",
        "# Define Sample Test Sentences\n",
        "sample_texts = [\n",
        "    \"You are the worst person ever!\",\n",
        "    \"I hope you have an amazing day!\",\n",
        "    \"I'll kill u!\",\n",
        "    \"This is absolute garbage!\",\n",
        "    \"What a wonderful place to visit!\",\n",
        "    \"I love you\",\n",
        "    \"Hello, my name is Anthony\",\n",
        "        \"Go to hell!\",\n",
        "    \"I hope your day is amazing!\",\n",
        "    \"I'm going to destroy you.\",\n",
        "    \"You are such a nice person.\",\n",
        "    \"Shut up, you idiot.\",\n",
        "    \"I disagree, but I respect your opinion.\",\n",
        "    \"I hope you die you stupid ugly bitch\",\n",
        "    \"you look fat\",\n",
        "    \"what size is your dress? XXXXXL?\",\n",
        "    \"you're not very smart are you?\",\n",
        "    \"how many dogs have you eaten, asian coworker\",\n",
        "    \"are you fucking homosexual\",\n",
        "    \"coworker, should I kill myself?\",\n",
        "]\n",
        "\n",
        "# Function to Tokenize Input Texts\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
        "\n",
        "# Tokenize Sample Inputs\n",
        "X_sample_tokens, X_sample_masks = tokenize_texts(sample_texts, tokenizer)\n",
        "X_sample_tokens, X_sample_masks = X_sample_tokens.to(device), X_sample_masks.to(device)\n",
        "print(\"Tokenization complete! Shape:\", X_sample_tokens.shape)\n",
        "\n",
        "# Run Model on Sample Texts\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_sample_tokens, attention_mask=X_sample_masks)  # Get raw logits\n",
        "    predictions = outputs.cpu().numpy()  # Apply sigmoid activation\n",
        "\n",
        "print(\"Processed Predictions:\", predictions)  # Debugging Step\n",
        "\n",
        "# Adjust Threshold to Reduce False Positives\n",
        "threshold = 0.5  # You can test different thresholds (0.5, 0.7, etc.)\n",
        "binary_predictions = (predictions >= threshold).astype(int)\n",
        "\n",
        "# Define Label Categories\n",
        "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "# Print Results\n",
        "for i, text in enumerate(sample_texts):\n",
        "    predicted_labels = [label_columns[j] for j in range(len(label_columns)) if binary_predictions[i][j] == 1]\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(\"Predicted Categories:\", predicted_labels if predicted_labels else [\"None\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tCpT5o7ZKVr",
        "outputId": "d49b0a1b-7b20-4876-ce9c-d185dec38838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model saved!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,  # Model to quantize\n",
        "    {torch.nn.Linear},  # Layers to apply quantization\n",
        "    dtype=torch.qint8  # Use 8-bit integer for compression\n",
        ")\n",
        "\n",
        "# Save the smaller model\n",
        "quantized_model_path = \"/content/drive/MyDrive/LINKUP_modelDIS_quantized.pth\"\n",
        "torch.save(quantized_model.state_dict(), quantized_model_path)\n",
        "print(\"Quantized model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVSvh0pC3mJR",
        "outputId": "5ee8c0dc-aaee-4fe3-d42b-90664d979249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-533a8ccf9129>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  quantized_model.load_state_dict(torch.load(quantized_model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, DistilBertModel\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Define the Model Structure (Must Match Original)\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
        "        return self.sigmoid(self.fc(pooled_output))\n",
        "\n",
        "# Initialize the Model\n",
        "output_dim = 6  # Ensure this matches your number of labels\n",
        "model = MultiLabelClassifier(output_dim)\n",
        "\n",
        "# Apply Quantization Again (Dynamic Quantization)\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,  # Model to quantize\n",
        "    {torch.nn.Linear},  # Layers to apply quantization\n",
        "    dtype=torch.qint8  # Use 8-bit integer for compression\n",
        ")\n",
        "\n",
        "# Load the Quantized State Dict\n",
        "quantized_model_path = \"/content/drive/MyDrive/LINKUP_modelDIS_quantized.pth\"\n",
        "quantized_model.load_state_dict(torch.load(quantized_model_path, map_location=device))\n",
        "\n",
        "# Move to device and set to eval mode\n",
        "quantized_model.to(device)\n",
        "quantized_model.eval()\n",
        "\n",
        "print(\"Quantized model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehHpcEuN3o8m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toit8wjR5K3F",
        "outputId": "eb931cc7-f2cf-4072-aca7-be8877cee1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_utils.py:410: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model loaded successfully!\n",
            "Tokenization complete! Shape: torch.Size([22, 128])\n",
            "Processed Predictions: [[9.9923337e-01 1.2398855e-04 1.6992586e-04 7.3362282e-04 8.7648499e-01\n",
            "  3.2024658e-05]\n",
            " [3.0926251e-04 1.6203623e-04 1.2005849e-04 3.3619934e-05 5.4767943e-04\n",
            "  9.7570613e-05]\n",
            " [9.9875534e-01 3.5899217e-04 4.2883487e-04 9.9368852e-01 6.9979258e-05\n",
            "  7.7432889e-04]\n",
            " [9.9904591e-01 3.0691741e-04 7.6916441e-02 9.6278432e-05 5.7004978e-05\n",
            "  1.4748689e-04]\n",
            " [3.8707658e-06 3.6496215e-04 1.2562229e-04 2.5409050e-04 8.6261109e-03\n",
            "  6.5809663e-04]\n",
            " [3.7465950e-03 2.9112294e-04 6.0185831e-04 1.1194328e-04 1.0241200e-04\n",
            "  1.2025934e-04]\n",
            " [6.8944551e-05 8.8962253e-05 2.5293037e-05 1.4394987e-04 2.7484904e-04\n",
            "  6.5459331e-05]\n",
            " [9.9825901e-01 1.4333529e-04 4.5933932e-01 9.9410427e-05 9.6026474e-01\n",
            "  2.1643129e-05]\n",
            " [9.9180434e-05 1.2967225e-04 3.6977217e-05 4.8254908e-05 2.4858158e-04\n",
            "  1.7467287e-04]\n",
            " [9.9967432e-01 7.9729309e-04 2.6794072e-04 9.6024060e-01 1.9639394e-04\n",
            "  5.6550687e-04]\n",
            " [2.5575198e-04 5.4469518e-05 1.9742451e-04 3.1861076e-05 1.0081783e-03\n",
            "  2.9117049e-05]\n",
            " [9.9997711e-01 3.8371047e-05 9.9610871e-01 5.9576345e-05 9.9957198e-01\n",
            "  2.2322261e-04]\n",
            " [2.5076688e-05 1.0462256e-04 1.4630234e-04 6.8637739e-05 1.1486588e-04\n",
            "  8.7394757e-05]\n",
            " [9.9997330e-01 1.6220959e-01 9.9533999e-01 9.0197265e-01 9.9998820e-01\n",
            "  5.6688437e-05]\n",
            " [9.9971133e-01 3.9980191e-04 4.5825876e-05 1.3137220e-03 9.5483363e-01\n",
            "  4.8500681e-03]\n",
            " [2.7157536e-01 3.0359044e-05 2.3194763e-03 2.4684507e-05 1.1684565e-05\n",
            "  1.5296086e-05]\n",
            " [9.2767572e-01 5.6050591e-05 2.6977146e-05 4.9867715e-05 1.3221217e-02\n",
            "  7.2306843e-06]\n",
            " [9.9874866e-01 3.5208643e-05 3.0585066e-05 1.4559562e-05 8.4220774e-02\n",
            "  2.3875868e-01]\n",
            " [9.9999249e-01 8.1277936e-04 9.9994767e-01 4.1696148e-05 9.9926311e-01\n",
            "  1.8627414e-02]\n",
            " [1.4617597e-01 1.7034541e-05 4.7575496e-04 1.0000504e-05 4.6600879e-05\n",
            "  1.3010068e-05]\n",
            " [9.9996257e-01 8.9990243e-04 2.0032073e-03 1.7253613e-03 5.4053043e-04\n",
            "  8.2911623e-01]\n",
            " [9.9998879e-01 4.4602723e-04 9.9996054e-01 8.2136001e-05 9.9967587e-01\n",
            "  8.8004458e-01]]\n",
            "\n",
            "Text: You are the worst person ever!\n",
            "Predicted Categories: ['toxic', 'insult']\n",
            "\n",
            "Text: I hope you have an amazing day!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I'll kill u!\n",
            "Predicted Categories: ['toxic', 'threat']\n",
            "\n",
            "Text: This is absolute garbage!\n",
            "Predicted Categories: ['toxic']\n",
            "\n",
            "Text: What a wonderful place to visit!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I love you\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Hello, my name is Anthony\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Go to hell!\n",
            "Predicted Categories: ['toxic', 'insult']\n",
            "\n",
            "Text: I hope your day is amazing!\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I'm going to destroy you.\n",
            "Predicted Categories: ['toxic', 'threat']\n",
            "\n",
            "Text: You are such a nice person.\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: Shut up, you idiot.\n",
            "Predicted Categories: ['toxic', 'obscene', 'insult']\n",
            "\n",
            "Text: I disagree, but I respect your opinion.\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: I hope you die you stupid ugly bitch\n",
            "Predicted Categories: ['toxic', 'obscene', 'threat', 'insult']\n",
            "\n",
            "Text: you look fat\n",
            "Predicted Categories: ['toxic', 'insult']\n",
            "\n",
            "Text: what size is your dress? XXXXXL?\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: you're not very smart are you?\n",
            "Predicted Categories: ['toxic']\n",
            "\n",
            "Text: how many dogs have you eaten, asian coworker\n",
            "Predicted Categories: ['toxic']\n",
            "\n",
            "Text: are you fucking homosexual\n",
            "Predicted Categories: ['toxic', 'obscene', 'insult']\n",
            "\n",
            "Text: coworker, should I kill myself?\n",
            "Predicted Categories: ['None']\n",
            "\n",
            "Text: i hate indians\n",
            "Predicted Categories: ['toxic', 'identity_hate']\n",
            "\n",
            "Text: mickey lookin ass nigga with yo bitchass ugly macaroni headass boi\n",
            "Predicted Categories: ['toxic', 'obscene', 'insult', 'identity_hate']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "# Force Model and Device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(\"Tokenizer loaded successfully!\")\n",
        "\n",
        "# Define Model Structure\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.sigmoid(self.fc(pooled_output))\n",
        "\n",
        "# Initialize Model Structure\n",
        "output_dim = 6\n",
        "model = MultiLabelClassifier(output_dim)\n",
        "\n",
        "# Apply Dynamic Quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Load Quantized Model Weights\n",
        "quantized_model_path = \"/content/drive/MyDrive/LINKUP_final.pth\"\n",
        "quantized_model.load_state_dict(torch.load(quantized_model_path, map_location=device))\n",
        "\n",
        "# Ensure Model is on CPU and in Evaluation Mode\n",
        "quantized_model.to(device)\n",
        "quantized_model.eval()\n",
        "\n",
        "print(\"Quantized model loaded successfully!\")\n",
        "\n",
        "# Sample Test Sentences\n",
        "sample_texts = [\n",
        "    \"You are the worst person ever!\",\n",
        "    \"I hope you have an amazing day!\",\n",
        "    \"I'll kill u!\",\n",
        "    \"This is absolute garbage!\",\n",
        "    \"What a wonderful place to visit!\",\n",
        "    \"I love you\",\n",
        "    \"Hello, my name is Anthony\",\n",
        "    \"Go to hell!\",\n",
        "    \"I hope your day is amazing!\",\n",
        "    \"I'm going to destroy you.\",\n",
        "    \"You are such a nice person.\",\n",
        "    \"Shut up, you idiot.\",\n",
        "    \"I disagree, but I respect your opinion.\",\n",
        "    \"I hope you die you stupid ugly bitch\",\n",
        "    \"you look fat\",\n",
        "    \"what size is your dress? XXXXXL?\",\n",
        "    \"you're not very smart are you?\",\n",
        "    \"how many dogs have you eaten, asian coworker\",\n",
        "    \"are you fucking homosexual\",\n",
        "    \"coworker, should I kill myself?\",\n",
        "]\n",
        "\n",
        "# Function to Tokenize Input Texts\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
        "\n",
        "# Tokenize and Move Inputs to CPU\n",
        "X_sample_tokens, X_sample_masks = tokenize_texts(sample_texts, tokenizer)\n",
        "X_sample_tokens, X_sample_masks = X_sample_tokens.to(device), X_sample_masks.to(device)\n",
        "\n",
        "# Print Tokenization Shape\n",
        "print(f\"Tokenization complete! Shape: {X_sample_tokens.shape}\")\n",
        "\n",
        "# Run Model on Sample Texts\n",
        "with torch.no_grad():\n",
        "    outputs = quantized_model(X_sample_tokens, attention_mask=X_sample_masks)\n",
        "    predictions = outputs.cpu().numpy()\n",
        "\n",
        "# Print Processed Predictions\n",
        "print(\"Processed Predictions:\", predictions)\n",
        "\n",
        "# Apply Threshold for Classification\n",
        "threshold = 0.5\n",
        "binary_predictions = (predictions >= threshold).astype(int)\n",
        "\n",
        "# Define Label Categories\n",
        "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "# Print Results\n",
        "for i, text in enumerate(sample_texts):\n",
        "    predicted_labels = [label_columns[j] for j in range(len(label_columns)) if binary_predictions[i][j] == 1]\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(\"Predicted Categories:\", predicted_labels if predicted_labels else [\"None\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZynbZEjGMF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrP-Qt1bGuGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5d3214e563b4212adce214338ded1ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_bce3a63611334d659805a3ef9ea961d2"
          }
        },
        "88a6a9d946f84f1fbea461ba961e6cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b951013c7e408cb84e5d4f57a90369",
            "placeholder": "​",
            "style": "IPY_MODEL_22543734c08149e7865a32553d314765",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "9ccac6d0e4754863bc98501e521f9493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c5c6ca5d896b4fa7aa1bfc7e867c6a6e",
            "placeholder": "​",
            "style": "IPY_MODEL_9b3cd07b72294948a1c7a5854f15a847",
            "value": ""
          }
        },
        "ccaa5c38502c437897d9e918926a8f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_574c79a2b2e14d808fcdc7cbff50d1ca",
            "style": "IPY_MODEL_1b0ede8519734344ad1713aea37c3ee7",
            "value": false
          }
        },
        "55da9c38e5704fae840a0292e6d68e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_64a50aa7acec467ebeaa8837bd710d3d",
            "style": "IPY_MODEL_aa08eb0bc4f8474c986157d6619136e0",
            "tooltip": ""
          }
        },
        "cf5f355b9bc54e4ab31f719d55bcbe78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba821c1e12ec414b894612a89fa5df89",
            "placeholder": "​",
            "style": "IPY_MODEL_08fb12287bbf4f82b7fd32c6164844fc",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "bce3a63611334d659805a3ef9ea961d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "38b951013c7e408cb84e5d4f57a90369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22543734c08149e7865a32553d314765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5c6ca5d896b4fa7aa1bfc7e867c6a6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b3cd07b72294948a1c7a5854f15a847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "574c79a2b2e14d808fcdc7cbff50d1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0ede8519734344ad1713aea37c3ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64a50aa7acec467ebeaa8837bd710d3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa08eb0bc4f8474c986157d6619136e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ba821c1e12ec414b894612a89fa5df89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08fb12287bbf4f82b7fd32c6164844fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f127e8a3d034b23a7e9eec90485e7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c825618c22b0409dab0e3a2fde6eed9f",
            "placeholder": "​",
            "style": "IPY_MODEL_7caa550cef5e4c69958bc025d2d01e8c",
            "value": "Connecting..."
          }
        },
        "c825618c22b0409dab0e3a2fde6eed9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7caa550cef5e4c69958bc025d2d01e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}